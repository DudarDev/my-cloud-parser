# -*- coding: utf-8 -*-

# --- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è ---
# –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Å–∞–π—Ç–∞ Swappa.com.
# –í–ï–†–°–ò–Ø 4.0: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ –≤–∏–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç API —Å–µ—Ä–≤–∏—Å–∞ ScrapingBee –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare.
#
# –ê–≤—Ç–æ—Ä: –Ø—Ä–æ—Å–ª–∞–≤ (–ø—Ä–∏ —Å–æ–¥–µ–π—Å—Ç–≤–∏–∏ Gemini)

# --- 1. –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ ---
import requests
import pandas as pd
from bs4 import BeautifulSoup
from docx import Document
import logging

# --- 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
SCRAPINGBEE_API_KEY = "9E3N7FKEJZZBQZ72RPSO7WF6DXO2XN6TM4XXZH3O2WS0T6ZYYV370BIZB1R20KPWT0FTHECHSARCDET7"
TARGET_URL = "https://swappa.com/listings/apple-iphone-13-pro-max"
OUTPUT_CSV_FILE = "swappa_iphone_report.csv"
OUTPUT_DOCX_FILE = "swappa_iphone_report.docx"


def fetch_page_html(api_key: str, url: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ ScrapingBee API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è HTML-–∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    logging.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ HTML —á–µ—Ä–µ–∑ ScrapingBee –¥–ª—è URL: {url}")
    
    response = requests.get(
        url='https://app.scrapingbee.com/api/v1/',
        params={
            'api_key': api_key,
            'url': url,
            'render_js': 'true', # –ì–æ–≤–æ—Ä–∏–º —Å–µ—Ä–≤–∏—Å—É –≤—ã–ø–æ–ª–Ω–∏—Ç—å JavaScript –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        }
    )
    
    if response.status_code == 200:
        logging.info("HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω.")
        return response.text
    else:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –°—Ç–∞—Ç—É—Å: {response.status_code}, –û—Ç–≤–µ—Ç: {response.text}")
        return None


def parse_html_data(html_content: str) -> list:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç HTML-–∫–æ–¥, —Ä–∞–∑–±–∏—Ä–∞–µ—Ç –µ–≥–æ —Å –ø–æ–º–æ—â—å—é BeautifulSoup –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ
    –∏–∑ –¢–ê–ë–õ–ò–ß–ù–û–ì–û –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
    """
    if not html_content:
        return []

    logging.info("–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–±–æ—Ä (–ø–∞—Ä—Å–∏–Ω–≥) HTML-–∫–æ–¥–∞...")
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # üî• –ù–û–í–´–ô –°–ï–õ–ï–ö–¢–û–† –¥–ª—è —Å—Ç—Ä–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    listing_elements = soup.select("table#listings_table tbody tr")
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(listing_elements)}")
    
    if not listing_elements:
        return []

    all_phones_data = []
    for i, row in enumerate(listing_elements, 1):
        try:
            # üî• –ù–û–í–´–ô –°–ü–û–°–û–ë –ü–û–ò–°–ö–ê: –∏—â–µ–º –≤—Å–µ —è—á–µ–π–∫–∏ (<td>) –≤ —Å—Ç—Ä–æ–∫–µ
            cells = row.select("td")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Å—Ç—Ä–æ–∫–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —è—á–µ–µ–∫, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
            if len(cells) < 14: # –í —Ç–∞–±–ª–∏—Ü–µ 14+ –∫–æ–ª–æ–Ω–æ–∫
                continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–æ—á–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É —è—á–µ–µ–∫
            price = cells[1].text.strip()
            carrier = cells[3].text.strip()
            color = cells[4].text.strip()
            storage = cells[5].text.strip()
            model = cells[6].text.strip()
            condition = cells[7].text.strip()
            battery = cells[8].text.strip()
            seller = cells[9].text.strip().split('\n')[0] # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è, –±–µ–∑ —Ä–µ–π—Ç–∏–Ω–≥–∞
            location = cells[10].text.strip()
            shipping = cells[12].text.strip()
            code = cells[13].text.strip()

            phone_data = {
                "price": price,
                "carrier": carrier,
                "color": color,
                "storage": storage,
                "model": model,
                "condition": condition,
                "battery": battery,
                "seller": seller,
                "location": location,
                "shipping": shipping,
                "code": code
            }
            all_phones_data.append(phone_data)
        except Exception as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫—É #{i}. –û—à–∏–±–∫–∞: {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue
            
    return all_phones_data


def save_to_csv(data: list, filename: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤ CSV-—Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é Pandas."""
    if not data:
        logging.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV.")
        return
    logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª {filename}...")
    try:
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8')
        logging.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ CSV: {e}")


def save_to_docx(data: list, filename: str):
    """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Word (.docx)."""
    if not data:
        logging.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ DOCX.")
        return
    logging.info(f"–°–æ–∑–¥–∞–Ω–∏–µ Word-–æ—Ç—á–µ—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {filename}...")
    try:
        document = Document()
        document.add_heading('–û—Ç—á–µ—Ç –ø–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º Swappa: iPhone 13 Pro Max', level=1)
        document.add_paragraph(f"–°–æ–±—Ä–∞–Ω–æ {len(data)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")
        
        for item in data:
            title = f"{item.get('storage')} {item.get('color')} ({item.get('condition')})"
            document.add_heading(title, level=3)
            
            p = document.add_paragraph()
            p.add_run('–¶–µ–Ω–∞: ').bold = True
            p.add_run(item.get('price', 'N/A'))
            
            p.add_run(' | –û–ø–µ—Ä–∞—Ç–æ—Ä: ').bold = True
            p.add_run(item.get('carrier', 'N/A'))
            
            p.add_run('\n–ü—Ä–æ–¥–∞–≤–µ—Ü: ').bold = True
            p.add_run(f"{item.get('seller', 'N/A')} ({item.get('location', 'N/A')})")

        document.save(filename)
        logging.info(f"–û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ DOCX: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, —É–ø—Ä–∞–≤–ª—è—é—â–∞—è –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º."""
    logging.info("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ v4.0 (API, Table Version)...")
    
    if "–í–ê–®_API_–ö–õ–Æ–ß" in SCRAPINGBEE_API_KEY:
        logging.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à API-–∫–ª—é—á –æ—Ç ScrapingBee –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é SCRAPINGBEE_API_KEY.")
        return

    html_content = fetch_page_html(SCRAPINGBEE_API_KEY, TARGET_URL)
    
    if html_content:
        scraped_data = parse_html_data(html_content)
        if scraped_data:
            save_to_csv(scraped_data, OUTPUT_CSV_FILE)
            save_to_docx(scraped_data, OUTPUT_DOCX_FILE)
        else:
            logging.warning("HTML –ø–æ–ª—É—á–µ–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä—ã.")
            with open("debug_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logging.info("HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_page.html –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    
    logging.info("–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    main()

